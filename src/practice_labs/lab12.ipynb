{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab12\n",
    "\n",
    "---\n",
    "\n",
    "## Task\n",
    "\n",
    "Для нахождения минимума сильно выпуклой функции (одной переменной) реализовать градиентный спуск (по одной из стратегий выбора шага) и метод Нестерова.\n",
    "\n",
    "Потестировать на нескольких функциях; сравнить количества итераций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import misc\n",
    "\n",
    "from plotly import graph_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_derivative(func, h):\n",
    "    return lambda x: misc.derivative(func, x, dx=h)\n",
    "\n",
    "def gradient_descent(func, x0, rate, rate_decrease, eps=10 ** (-8), max_iter=100):\n",
    "    \"\"\"\n",
    "    Gradient descent using step fragmentation strategy\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    points = [x0]\n",
    "    df = get_derivative(func, 1e-10)\n",
    "    grad = df(x0)\n",
    "    \n",
    "    while abs(grad) > eps:\n",
    "        grad = df(x)\n",
    "        \n",
    "        x = x - rate * grad\n",
    "        points.append(x)\n",
    "        \n",
    "        if len(points) > max_iter:\n",
    "            break\n",
    "    \n",
    "        rate = (1 - rate_decrease) * rate\n",
    "    \n",
    "    return x, points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод Нестерова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov_method(f, y0, jitter, eps=10**(-8), max_iter=100):\n",
    "    x = y0\n",
    "    y = y0\n",
    "    a = 1\n",
    "    df = get_derivative(f, 1e-10)\n",
    "    z = y - jitter\n",
    "    alpha = abs((y - z) / (df(y) - df(z)))\n",
    "    points = [x]\n",
    "\n",
    "    while abs(df(points[-1])) > eps:\n",
    "        i = 0\n",
    "        grad = df(y)\n",
    "        \n",
    "        while f(y) - f(y - 2 ** (-i) * alpha * grad) < 2 ** (- i - 1) * alpha * grad ** 2:\n",
    "            i += 1\n",
    "            if i >= 10**6:\n",
    "                print(f'Jump overflow at iteration {len(points)}')\n",
    "                return x, points\n",
    "\n",
    "        alpha = 2 ** (-i) * alpha\n",
    "        x = y - alpha * grad\n",
    "        points.append(x)\n",
    "        \n",
    "        if len(points) > max_iter:\n",
    "            break\n",
    "        prev_a = a\n",
    "        a = (1 + np.sqrt(4 * prev_a ** 2 + 1)) / 2\n",
    "        y = x + (prev_a - 1) * (x - points[-2]) / a\n",
    "    \n",
    "    return x, points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental reseach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(func, segment, rate, x0):\n",
    "    a, b = segment\n",
    "    grid = np.linspace(a, b, num=300)\n",
    "\n",
    "    x_min, points = gradient_descent(func, x0, rate, rate_decrease=0.02)\n",
    "    \n",
    "    fig1 = graph_objects.Figure()\n",
    "    fig1.add_trace(graph_objects.Scatter(x=points, y=[func(x) for x in points],mode='lines+markers'),)\n",
    "    fig1.add_trace(graph_objects.Scatter(x=grid, y=[func(x) for x in grid], mode='lines'))\n",
    "\n",
    "    fig1.update_layout(autosize=False, template=\"simple_white\", showlegend=False)\n",
    "    fig1.show()\n",
    "\n",
    "    print(\"Gradient descent: step fragmentation\")\n",
    "    print(f\"Found minimum: {x_min}\")\n",
    "    print(f\"Iterations: {len(points)}\")\n",
    "\n",
    "    x_min, points = nesterov_method(func, y0=0, jitter=0.1)\n",
    "\n",
    "    fig2 = graph_objects.Figure()\n",
    "    fig2.add_trace(graph_objects.Scatter(x=points, y=[func(x) for x in points],mode='lines+markers'))\n",
    "    fig2.add_trace(graph_objects.Scatter(x=grid, y=[func(x) for x in grid], mode='lines'))\n",
    "\n",
    "    fig2.update_layout(autosize=False, template=\"simple_white\", showlegend=False)\n",
    "    fig2.show()\n",
    "\n",
    "    print(\"Nesterov's method\")\n",
    "    print(f\"Found minimum: {x_min}\")\n",
    "    print(f\"Iterations: {len(points)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x * (x + 1) \n",
    "rate = 0.7\n",
    "x0 = -2\n",
    "\n",
    "experiment(f, (-10, 10), rate, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: 3 * np.sin(x)\n",
    "rate = 0.4\n",
    "x0 = 0\n",
    "\n",
    "experiment(f, (-4, 4), rate, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x * x * x - 5 * x\n",
    "rate = 0.2\n",
    "x0 = 0\n",
    "\n",
    "experiment(f, (-2, 2), rate, x0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "baaf916ca6c4ce979e8147ddecfa27bf3f96ad18e9f19ff9b194c54fcb57b5ff"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
